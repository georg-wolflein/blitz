{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"  # needs to be set *before* triton is imported\n",
    "\n",
    "\n",
    "def check_tensors_gpu_ready(*tensors):\n",
    "    for t in tensors:\n",
    "        assert t.is_contiguous, \"A tensor is not contiguous\"\n",
    "        if not os.environ.get(\"TRITON_INTERPRET\") == \"1\":\n",
    "            assert t.is_cuda, \"A tensor is not on cuda\"\n",
    "\n",
    "\n",
    "def test_pid_conds(conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n",
    "    \"\"\"Test if condition on pids are fulfilled\n",
    "    E.g.:\n",
    "        '=0'  checks that pid_0 == 0\n",
    "        ',>1' checks that pid_1 > 1\n",
    "        '>1,=0' checks that pid_0 > 1 and pid_1 == 0\n",
    "    \"\"\"\n",
    "    pids = pid_0[0], pid_1[0], pid_2[0]\n",
    "    conds = conds.replace(\" \", \"\").split(\",\")\n",
    "    for i, (cond, pid) in enumerate(zip(conds, pids)):\n",
    "        if cond == \"\":\n",
    "            continue\n",
    "        op, threshold = cond[0], int(cond[1:])\n",
    "        if op not in [\"<\", \">\", \">=\", \"<=\", \"=\", \"!=\"]:\n",
    "            raise ValueError(f\"Rules may only use these ops: '<','>','>=','<=','=', '!='. Invalid rule: '{condition}'.\")\n",
    "        op = \"==\" if op == \"=\" else op\n",
    "        if not eval(f\"{pid} {op} {threshold}\"):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "assert test_pid_conds(\"\")\n",
    "assert test_pid_conds(\">0\", [1], [1])\n",
    "assert not test_pid_conds(\">0\", [0], [1])\n",
    "assert test_pid_conds(\"=0,=1\", [0], [1], [0])\n",
    "\n",
    "\n",
    "def breakpoint_if(conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n",
    "    \"\"\"Stop kernel, if any condition of pids is fulfilled\"\"\"\n",
    "    if test_pid_conds(conds, pid_0, pid_1, pid_2):\n",
    "        set_trace()\n",
    "\n",
    "\n",
    "def print_if(txt, conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n",
    "    \"\"\"Print txt, if any condition of pids is fulfilled\"\"\"\n",
    "    if test_pid_conds(conds, pid_0, pid_1, pid_2):\n",
    "        print(txt)\n",
    "\n",
    "\n",
    "def cdiv(a, b):\n",
    "    return (a + b - 1) // b\n",
    "\n",
    "\n",
    "assert cdiv(10, 2) == 5\n",
    "assert cdiv(10, 3) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from typing import Optional\n",
    "\n",
    "from attn_torch import torch_scaled_dot_product_attention\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch implementation of FlashAttention\n",
    "I follow the notation of Algorithm 1: https://arxiv.org/pdf/2205.14135.pdf\n",
    "\n",
    "Helpful resources:\n",
    "- [triton implementation of FA1](https://github.com/openai/triton/blob/fdf1c1f2a1f4de37ce1fb31316d53004d6e7e98c/python/tutorials/06-fused-attention.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def pad_to(x, dim, size, value=0.0):\n",
    "    \"\"\"Append padding to the input tensor x to match the target size along the given dimension.\"\"\"\n",
    "    pad_size = size - x.size(dim)\n",
    "    if pad_size > 0:\n",
    "        pad_dims = list(x.shape)\n",
    "        pad_dims[dim] = pad_size\n",
    "        pad = torch.full(pad_dims, value, dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([x, pad], dim=dim)\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def torch_flash_attention_kernel(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    softmax_scale: Optional[float] = None,\n",
    "    B_r: int = 128,\n",
    "    B_c: int = 128,\n",
    "):\n",
    "    \"\"\"Flash attention kernel implementation using torch operations.\n",
    "\n",
    "    This implementation closely follows Algorithm 1 in the FlashAttention paper: https://arxiv.org/pdf/2205.14135.pdf.\n",
    "    The only difference is that we perform the attention scaling by sqrt(d) as part of the computation.\n",
    "\n",
    "    This implementation is not intended to be used; it is only for reference and testing purposes.\n",
    "\n",
    "    Args:\n",
    "        Q: Queries tensor of shape [Z, H, N, D]\n",
    "        K: Keys tensor of shape [Z, H, N, D]\n",
    "        V: Values tensor of shape [Z, H, N, D]\n",
    "        B_r: The block size for the rows\n",
    "        B_c: The block size for the columns\n",
    "    \"\"\"\n",
    "\n",
    "    Z, H, N, D = Q.shape\n",
    "    dtype = Q.dtype\n",
    "    device = Q.device\n",
    "\n",
    "    softmax_scale = softmax_scale or 1.0 / (D**0.5)\n",
    "\n",
    "    def inner(Q, K, V):\n",
    "\n",
    "        # 2. Initialize O in HBM\n",
    "        O = torch.zeros(N, D, device=device, dtype=dtype)  # [N, D]\n",
    "\n",
    "        # 3. Divide Q into T_r blocks of size [B_r, D] each\n",
    "        T_r = cdiv(N, B_r)\n",
    "        Q = list(torch.split(Q, B_r))  # [T_r, B_r, D]\n",
    "\n",
    "        # 3. Divide K, V into T_c blocks of size [B_c, d] each\n",
    "        T_c = cdiv(N, B_c)\n",
    "        K = list(torch.split(K, B_c))  # [T_c, B_c, D]\n",
    "        V = list(torch.split(V, B_c))  # [T_c, B_c, D]\n",
    "\n",
    "        # 4. Divide O into T_r blocks of size [B_r, D] each\n",
    "        O = list()\n",
    "\n",
    "        # 7. Outer loop (NOTE: in Algorithm 1, this is the inner loop)\n",
    "        for i in range(T_r):\n",
    "            # 8. Load Q_i, O_i, l_i, m_i into SRAM\n",
    "            Q_i = Q[i]  # [B_r, D]\n",
    "            Q_i = pad_to(Q_i, 0, B_r)  # simulate padding\n",
    "\n",
    "            # 2. and 4. Divide l, m into T_r blocks of size [B_r] each\n",
    "            l_i = torch.zeros(B_r, device=device, dtype=dtype)  # [B_r]\n",
    "            m_i = torch.full((B_r,), float(\"-inf\"), device=device, dtype=dtype)  # [B_r]\n",
    "            O_i = torch.zeros(B_r, D, device=device, dtype=dtype)  # [B_r, B_c]\n",
    "\n",
    "            # 5. Inner loop (NOTE: in Algorithm 1, this is the outer loop)\n",
    "            for j in range(T_c):\n",
    "                # 6. Load K_j, V_j into SRAM\n",
    "                K_j = K[j]  # [B_c, d]\n",
    "                V_j = V[j]  # [B_c, d]\n",
    "\n",
    "                K_j = pad_to(K_j, 0, B_c)  # simulate padding\n",
    "                V_j = pad_to(V_j, 0, B_c)  # simulate padding\n",
    "\n",
    "                # 9. On chip, compute S_ij = Q_i @ K_j^T\n",
    "                S_ij = Q_i @ K_j.T  # [B_r, B_c]\n",
    "\n",
    "                # 9a. Scale by sqrt(d) (not in the paper, but part of the attention formula)\n",
    "                S_ij = S_ij * softmax_scale\n",
    "\n",
    "                # 9b. Mask out-of-bounds elements\n",
    "                S_ij = torch.where(torch.arange(B_c, device=device).unsqueeze(0) + j * B_c < N, S_ij, -float(\"inf\"))\n",
    "\n",
    "                # 10. On chip, compute mtilde_ij = rowmax(S_ij)\n",
    "                mtilde_ij = S_ij.max(dim=1).values  # [B_r]\n",
    "\n",
    "                # 10. On chip, compute Ptilde_ij = exp(S_ij - mtilde_ij)\n",
    "                Ptilde_ij = torch.exp(S_ij - mtilde_ij.unsqueeze(1))  # [B_r, B_c]\n",
    "\n",
    "                # 11. On chip, compute ltilde_ij = rowsum(Ptilde_ij)\n",
    "                ltilde_ij = Ptilde_ij.sum(dim=1)  # [B_r]\n",
    "\n",
    "                # 11. On chip, compute mnew_i = max(m_i, mtilde_ij)\n",
    "                mnew_i = torch.maximum(m_i, mtilde_ij)  # [B_r]\n",
    "\n",
    "                # 11. On chip, compute lnew_i = exp(m_i - mnew_i) * l_i + exp(mtilde_ij - mnew_i) * ltilde_ij\n",
    "                alpha = torch.exp(m_i - mnew_i)  # [B_r]\n",
    "                beta = torch.exp(mtilde_ij - mnew_i)  # [B_r]\n",
    "                lnew_i = alpha * l_i + beta * ltilde_ij  # [B_r]\n",
    "\n",
    "                # 12. Write O_i = diag(lnew_i)^-1 (diag(l_i) exp(m_i - mnew_i) O_i + exp(mtilde_ij - mnew_i) Ptilde_ij V_j) to HBM\n",
    "                P_scale = beta / lnew_i  # [B_r]\n",
    "                O_scale = l_i / lnew_i * alpha  # [B_r]\n",
    "                O_i = O_i * O_scale.unsqueeze(1) + (Ptilde_ij * P_scale.unsqueeze(1)) @ V_j\n",
    "\n",
    "                # 13. Write l_i = lnew_i to HBM\n",
    "                l_i = lnew_i\n",
    "\n",
    "                # 13. Write m_i = mnew_i to HBM\n",
    "                m_i = mnew_i\n",
    "\n",
    "            O.append(O_i)  # write to HBM\n",
    "\n",
    "        O = torch.cat(O)\n",
    "        O = O[:N]  # remove padding\n",
    "        return O\n",
    "\n",
    "    # Run inner across Z, H dimensions\n",
    "    O = torch.stack([torch.stack([inner(Q[z, h], K[z, h], V[z, h]) for h in range(H)]) for z in range(Z)])\n",
    "    return O\n",
    "\n",
    "\n",
    "Z = 6  # batch size\n",
    "H = 2  # num heads\n",
    "N = 8  # sequence length\n",
    "D = 4  # embed dim (head dim)\n",
    "\n",
    "# M = 128  # on-chip SRAM size\n",
    "# B_c = cdiv(M, 4 * d)  # block size\n",
    "# B_r = min(B_c, d)\n",
    "\n",
    "B_c = B_r = 4\n",
    "\n",
    "\n",
    "Q = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "K = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "V = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "output_flash = torch_flash_attention_kernel(Q, K, V, B_r=B_r, B_c=B_c)\n",
    "# print(F.scaled_dot_product_attention(Q, K, V))\n",
    "output_torch = torch_scaled_dot_product_attention(Q, K, V)\n",
    "print(torch.allclose(output_flash, output_torch, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(    0.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def triton_flash_attention_kernel(\n",
    "    Q_ptr,\n",
    "    K_ptr,\n",
    "    V_ptr,\n",
    "    O_ptr,\n",
    "    stride_Q0,\n",
    "    stride_Q1,\n",
    "    stride_K0,\n",
    "    stride_K1,\n",
    "    stride_V0,\n",
    "    stride_V1,\n",
    "    stride_O0,\n",
    "    stride_O1,\n",
    "    N: int,\n",
    "    D: int,\n",
    "    softmax_scale: float,\n",
    "    B_r: tl.constexpr,\n",
    "    B_c: tl.constexpr,\n",
    "    B_d: tl.constexpr,\n",
    "    allow_tf32: tl.constexpr = False,\n",
    "):\n",
    "    assert D == B_d\n",
    "    i = tl.program_id(0)\n",
    "    zh = tl.program_id(1)\n",
    "\n",
    "    # 8. Load Q_i, O_i, l_i, m_i into SRAM\n",
    "    Q_i_ptrs = tl.make_block_ptr(\n",
    "        base=Q_ptr,\n",
    "        shape=(N, D),\n",
    "        strides=(stride_Q0, stride_Q1),\n",
    "        offsets=(i * B_r, 0),\n",
    "        block_shape=(B_r, B_d),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    Q_i = tl.load(Q_i_ptrs)  # [B_r, D]\n",
    "\n",
    "    O_i = tl.zeros((B_r, B_c), dtype=Q_i.dtype)  # [B_r, B_c]\n",
    "    l_i = tl.zeros((B_r,), dtype=Q_i.dtype)  # [B_r]\n",
    "    m_i = tl.full((B_r,), -float(\"inf\"), dtype=Q_i.dtype)  # [B_r]\n",
    "\n",
    "    # 3. Divide K, V into T_c blocks of size [B_c, D] each\n",
    "    T_c = tl.cdiv(N, B_c)\n",
    "\n",
    "    K_j_ptrs = tl.make_block_ptr(\n",
    "        base=K_ptr,\n",
    "        shape=(N, D),\n",
    "        strides=(stride_K0, stride_K1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(B_c, B_d),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    V_j_ptrs = tl.make_block_ptr(\n",
    "        base=V_ptr,\n",
    "        shape=(N, D),\n",
    "        strides=(stride_V0, stride_V1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(B_c, B_d),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "\n",
    "    # Inner loop (NOTE: in Algorithm 1, this is the outer loop; Algorithm 1's inner loop is the outer loop here via tl.program_id(0))\n",
    "    for j in range(T_c):\n",
    "        # 3. Divide K, V into T_c blocks of size [B_c, D] each\n",
    "        # 6. Load K_j, V_j into SRAM\n",
    "        K_j = tl.load(K_j_ptrs, boundary_check=(0, 1))  # [B_c, D]\n",
    "        V_j = tl.load(V_j_ptrs, boundary_check=(0, 1))  # [B_c, D]\n",
    "\n",
    "        K_j = tl.trans(K_j)  # [B_c, D]\n",
    "\n",
    "        # 9. On chip, compute S_ij = Q_i @ K_j^T\n",
    "        S_ij = tl.dot(Q_i, K_j, allow_tf32=allow_tf32)  # [B_r, B_c] # NOTE: K_j is already loaded in its transpose\n",
    "\n",
    "        # 9a. Scale by sqrt(d) (not in the paper, but part of the attention formula)\n",
    "        S_ij = S_ij * softmax_scale\n",
    "\n",
    "        # 9b. Mask out-of-bounds elements\n",
    "        rows = j * B_c + tl.arange(0, B_c)\n",
    "        S_ij = tl.where((rows[None, :] < N), S_ij, -float(\"inf\"))\n",
    "\n",
    "        # 10. On chip, compute mtilde_ij = rowmax(S_ij)\n",
    "        mtilde_ij = tl.max(S_ij, axis=1)  # [B_r]\n",
    "\n",
    "        # 10. On chip, compute Ptilde_ij = exp(S_ij - mtilde_ij)\n",
    "        Ptilde_ij = tl.exp(S_ij - mtilde_ij[:, None])  # [B_r, B_c]\n",
    "\n",
    "        # 11. On chip, compute ltilde_ij = rowsum(Ptilde_ij)\n",
    "        ltilde_ij = tl.sum(Ptilde_ij, axis=1)  # [B_r]\n",
    "\n",
    "        # 11. On chip, compute mnew_i = max(m_i, mtilde_ij)\n",
    "        mnew_i = tl.maximum(m_i, mtilde_ij)  # [B_r]\n",
    "\n",
    "        # 11. On chip, compute lnew_i = exp(m_i - mnew_i) * l_i + exp(mtilde_ij - mnew_i) * ltilde_ij\n",
    "        alpha = tl.exp(m_i - mnew_i)  # [B_r]\n",
    "        beta = tl.exp(mtilde_ij - mnew_i)  # [B_r]\n",
    "        lnew_i = alpha * l_i + beta * ltilde_ij  # [B_r]\n",
    "\n",
    "        # 12. Write O_i = diag(lnew_i)^-1 (diag(l_i) exp(m_i - mnew_i) O_i + exp(mtilde_ij - mnew_i) Ptilde_ij V_j) to HBM\n",
    "        P_scale = beta / lnew_i  # [B_r]\n",
    "        O_scale = l_i / lnew_i * alpha  # [B_r]\n",
    "        O_i = O_i * O_scale[:, None] + tl.dot(Ptilde_ij * P_scale[:, None], V_j, allow_tf32=allow_tf32)\n",
    "\n",
    "        # 13. Write l_i = lnew_i to HBM\n",
    "        l_i = lnew_i\n",
    "\n",
    "        # 13. Write m_i = mnew_i to HBM\n",
    "        m_i = mnew_i\n",
    "\n",
    "        # Advance block pointers to the next block\n",
    "        K_j_ptrs = K_j_ptrs.advance((B_c, 0))\n",
    "        V_j_ptrs = V_j_ptrs.advance((B_c, 0))\n",
    "\n",
    "    # 12. Write O_i to HBM\n",
    "    O_i_ptrs = tl.make_block_ptr(\n",
    "        base=O_ptr,\n",
    "        shape=(N, D),\n",
    "        strides=(stride_O0, stride_O1),\n",
    "        offsets=(i * B_r, 0),\n",
    "        block_shape=(B_r, B_d),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    tl.store(O_i_ptrs, O_i)\n",
    "\n",
    "\n",
    "def triton_flash_attention(Q, K, V, B_r, B_c):\n",
    "    N, D = Q.shape\n",
    "    dtype = Q.dtype\n",
    "\n",
    "    # 2. Initialize O, l, m in HBM\n",
    "    O = torch.zeros(N, D, device=Q.device, dtype=dtype)  # [N, d]\n",
    "\n",
    "    B_d = D\n",
    "\n",
    "    T_r = cdiv(N, B_r)\n",
    "    softmax_scale = 1.0 / D**0.5\n",
    "\n",
    "    triton_flash_attention_kernel[(T_r, 1, 1)](\n",
    "        Q,\n",
    "        K,\n",
    "        V,\n",
    "        O,\n",
    "        Q.stride(0),\n",
    "        Q.stride(1),\n",
    "        K.stride(0),\n",
    "        K.stride(1),\n",
    "        V.stride(0),\n",
    "        V.stride(1),\n",
    "        O.stride(0),\n",
    "        O.stride(1),\n",
    "        N,\n",
    "        D,\n",
    "        softmax_scale,\n",
    "        B_r,\n",
    "        B_c,\n",
    "        B_d,\n",
    "    )\n",
    "    return O\n",
    "\n",
    "\n",
    "N = 34  # batch size\n",
    "D = 16  # embed dim (head dim)\n",
    "M = 128  # on-chip SRAM size\n",
    "B_r = B_c = 16\n",
    "B_d = D\n",
    "\n",
    "# B_c = cdiv(M, 4 * d)  # block size\n",
    "# B_r = min(B_c, d)\n",
    "\n",
    "\n",
    "# N = 2\n",
    "# D = 16\n",
    "# B_r = B_c = 16\n",
    "# B_d = D\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "Q = torch.randn(N, D, dtype=torch.float32, device=\"cuda\")\n",
    "K = torch.randn(N, D, dtype=torch.float32, device=\"cuda\")\n",
    "V = torch.randn(N, D, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "output_triton = triton_flash_attention(Q, K, V, B_r=B_r, B_c=B_c)\n",
    "# print(F.scaled_dot_product_attention(Q, K, V))\n",
    "# output_torch = torch_scaled_dot_product_attention(Q, K, V)\n",
    "torch.cuda.synchronize()\n",
    "Q, K, V = Q.unsqueeze(0).unsqueeze(0), K.unsqueeze(0).unsqueeze(0), V.unsqueeze(0).unsqueeze(0)\n",
    "output_torch = torch_flash_attention_kernel(Q, K, V, B_r=B_r, B_c=B_c).squeeze(0).squeeze(0)\n",
    "print(torch.allclose(output_triton, output_torch, atol=1e-6))\n",
    "print((output_torch - output_triton).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
