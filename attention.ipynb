{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"  # needs to be set *before* triton is imported\n",
    "\n",
    "\n",
    "def check_tensors_gpu_ready(*tensors):\n",
    "    for t in tensors:\n",
    "        assert t.is_contiguous, \"A tensor is not contiguous\"\n",
    "        if not os.environ.get(\"TRITON_INTERPRET\") == \"1\":\n",
    "            assert t.is_cuda, \"A tensor is not on cuda\"\n",
    "\n",
    "\n",
    "def test_pid_conds(conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n",
    "    \"\"\"Test if condition on pids are fulfilled\n",
    "    E.g.:\n",
    "        '=0'  checks that pid_0 == 0\n",
    "        ',>1' checks that pid_1 > 1\n",
    "        '>1,=0' checks that pid_0 > 1 and pid_1 == 0\n",
    "    \"\"\"\n",
    "    pids = pid_0[0], pid_1[0], pid_2[0]\n",
    "    conds = conds.replace(\" \", \"\").split(\",\")\n",
    "    for i, (cond, pid) in enumerate(zip(conds, pids)):\n",
    "        if cond == \"\":\n",
    "            continue\n",
    "        op, threshold = cond[0], int(cond[1:])\n",
    "        if op not in [\"<\", \">\", \">=\", \"<=\", \"=\", \"!=\"]:\n",
    "            raise ValueError(f\"Rules may only use these ops: '<','>','>=','<=','=', '!='. Invalid rule: '{condition}'.\")\n",
    "        op = \"==\" if op == \"=\" else op\n",
    "        if not eval(f\"{pid} {op} {threshold}\"):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "assert test_pid_conds(\"\")\n",
    "assert test_pid_conds(\">0\", [1], [1])\n",
    "assert not test_pid_conds(\">0\", [0], [1])\n",
    "assert test_pid_conds(\"=0,=1\", [0], [1], [0])\n",
    "\n",
    "\n",
    "def breakpoint_if(conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n",
    "    \"\"\"Stop kernel, if any condition of pids is fulfilled\"\"\"\n",
    "    if test_pid_conds(conds, pid_0, pid_1, pid_2):\n",
    "        set_trace()\n",
    "\n",
    "\n",
    "def print_if(txt, conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n",
    "    \"\"\"Print txt, if any condition of pids is fulfilled\"\"\"\n",
    "    if test_pid_conds(conds, pid_0, pid_1, pid_2):\n",
    "        print(txt)\n",
    "\n",
    "\n",
    "def cdiv(a, b):\n",
    "    return (a + b - 1) // b\n",
    "\n",
    "\n",
    "assert cdiv(10, 2) == 5\n",
    "assert cdiv(10, 3) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from typing import Optional\n",
    "\n",
    "from attn_torch import torch_scaled_dot_product_attention\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch implementation of FlashAttention\n",
    "I follow the notation of Algorithm 1: https://arxiv.org/pdf/2205.14135.pdf\n",
    "\n",
    "Helpful resources:\n",
    "- [triton implementation of FA1](https://github.com/openai/triton/blob/fdf1c1f2a1f4de37ce1fb31316d53004d6e7e98c/python/tutorials/06-fused-attention.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "O_i.shape=torch.Size([2, 16]), O_scale.shape=torch.Size([2]), Ptilde_ij.shape=torch.Size([2, 4]), P_scale.shape=torch.Size([2]), V_j.shape=torch.Size([4, 16])\n",
      "O_i.shape=torch.Size([2, 16]), O_scale.shape=torch.Size([2]), Ptilde_ij.shape=torch.Size([2, 4]), P_scale.shape=torch.Size([2]), V_j.shape=torch.Size([4, 16])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def pad_to(x, dim, size, value=0.0):\n",
    "    \"\"\"Append padding to the input tensor x to match the target size along the given dimension.\"\"\"\n",
    "    pad_size = size - x.size(dim)\n",
    "    if pad_size > 0:\n",
    "        pad_dims = list(x.shape)\n",
    "        pad_dims[dim] = pad_size\n",
    "        pad = torch.full(pad_dims, value, dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([x, pad], dim=dim)\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def torch_flash_attention_kernel(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    softmax_scale: Optional[float] = None,\n",
    "    B_r: int = 128,\n",
    "    B_c: int = 128,\n",
    "):\n",
    "    \"\"\"Flash attention kernel implementation using torch operations.\n",
    "\n",
    "    This implementation closely follows Algorithm 1 in the FlashAttention paper: https://arxiv.org/pdf/2205.14135.pdf.\n",
    "    The only difference is that we perform the attention scaling by sqrt(d) as part of the computation.\n",
    "\n",
    "    This implementation is not intended to be used; it is only for reference and testing purposes.\n",
    "\n",
    "    Args:\n",
    "        Q: Queries tensor of shape [N, d]\n",
    "        K: Keys tensor of shape [N, d]\n",
    "        V: Values tensor of shape [N, d]\n",
    "        B_r: The block size for the rows\n",
    "        B_c: The block size for the columns\n",
    "    \"\"\"\n",
    "\n",
    "    N, d = Q.shape\n",
    "    dtype = Q.dtype\n",
    "    device = Q.device\n",
    "\n",
    "    softmax_scale = softmax_scale or 1.0 / (d**0.5)\n",
    "\n",
    "    # 2. Initialize O in HBM\n",
    "    O = torch.zeros(N, d, device=device, dtype=dtype)  # [N, d]\n",
    "\n",
    "    # 3. Divide Q into T_r blocks of size [B_r, d] each\n",
    "    T_r = cdiv(N, B_r)\n",
    "    Q = list(torch.split(Q, B_r))  # [T_r, B_r, d]\n",
    "\n",
    "    # 3. Divide K, V into T_c blocks of size [B_c, d] each\n",
    "    T_c = cdiv(N, B_c)\n",
    "    K = list(torch.split(K, B_c))  # [T_c, B_c, d]\n",
    "    V = list(torch.split(V, B_c))  # [T_c, B_c, d]\n",
    "\n",
    "    # 4. Divide O into T_r blocks of size [B_r, d] each\n",
    "    O = list()\n",
    "\n",
    "    # 7. Outer loop (NOTE: in Algorithm 1, this is the inner loop)\n",
    "    for i in range(T_r):\n",
    "        # 8. Load Q_i, O_i, l_i, m_i into SRAM\n",
    "        Q_i = Q[i]  # [B_r, d]\n",
    "        Q_i = pad_to(Q_i, 0, B_r)  # simulate padding\n",
    "\n",
    "        # 2. and 4. Divide l, m into T_r blocks of size [B_r] each\n",
    "        l_i = torch.zeros(B_r, device=device, dtype=dtype)  # [B_r]\n",
    "        m_i = torch.full((B_r,), float(\"-inf\"), device=device, dtype=dtype)  # [B_r]\n",
    "        O_i = torch.zeros(B_r, d, device=device, dtype=dtype)  # [B_r, B_c]\n",
    "\n",
    "        # 5. Inner loop (NOTE: in Algorithm 1, this is the outer loop)\n",
    "        for j in range(T_c):\n",
    "            # 6. Load K_j, V_j into SRAM\n",
    "            K_j = K[j]  # [B_c, d]\n",
    "            V_j = V[j]  # [B_c, d]\n",
    "\n",
    "            K_j = pad_to(K_j, 0, B_c)  # simulate padding\n",
    "            V_j = pad_to(V_j, 0, B_c)  # simulate padding\n",
    "\n",
    "            # 9. On chip, compute S_ij = Q_i @ K_j^T\n",
    "            S_ij = Q_i @ K_j.T  # [B_r, B_c]\n",
    "\n",
    "            # 9a. Scale by sqrt(d) (not in the paper, but part of the attention formula)\n",
    "            S_ij = S_ij * softmax_scale\n",
    "\n",
    "            # 9b. Mask out-of-bounds elements\n",
    "            S_ij = torch.where(torch.arange(B_c, device=device).unsqueeze(0) + j * B_c < N, S_ij, -float(\"inf\"))\n",
    "\n",
    "            # 10. On chip, compute mtilde_ij = rowmax(S_ij)\n",
    "            mtilde_ij = S_ij.max(dim=1).values  # [B_r]\n",
    "\n",
    "            # 10. On chip, compute Ptilde_ij = exp(S_ij - mtilde_ij)\n",
    "            Ptilde_ij = torch.exp(S_ij - mtilde_ij.unsqueeze(1))  # [B_r, B_c]\n",
    "\n",
    "            # 11. On chip, compute ltilde_ij = rowsum(Ptilde_ij)\n",
    "            ltilde_ij = Ptilde_ij.sum(dim=1)  # [B_r]\n",
    "\n",
    "            # 11. On chip, compute mnew_i = max(m_i, mtilde_ij)\n",
    "            mnew_i = torch.maximum(m_i, mtilde_ij)  # [B_r]\n",
    "\n",
    "            # 11. On chip, compute lnew_i = exp(m_i - mnew_i) * l_i + exp(mtilde_ij - mnew_i) * ltilde_ij\n",
    "            alpha = torch.exp(m_i - mnew_i)  # [B_r]\n",
    "            beta = torch.exp(mtilde_ij - mnew_i)  # [B_r]\n",
    "            lnew_i = alpha * l_i + beta * ltilde_ij  # [B_r]\n",
    "\n",
    "            # 12. Write O_i = diag(lnew_i)^-1 (diag(l_i) exp(m_i - mnew_i) O_i + exp(mtilde_ij - mnew_i) Ptilde_ij V_j) to HBM\n",
    "            P_scale = beta / lnew_i  # [B_r]\n",
    "            O_scale = l_i / lnew_i * alpha  # [B_r]\n",
    "            print(f\"{O_i.shape=}, {O_scale.shape=}, {Ptilde_ij.shape=}, {P_scale.shape=}, {V_j.shape=}\")\n",
    "            O_i = O_i * O_scale.unsqueeze(1) + (Ptilde_ij * P_scale.unsqueeze(1)) @ V_j\n",
    "\n",
    "            # 13. Write l_i = lnew_i to HBM\n",
    "            l_i = lnew_i\n",
    "\n",
    "            # 13. Write m_i = mnew_i to HBM\n",
    "            m_i = mnew_i\n",
    "\n",
    "        O.append(O_i)  # write to HBM\n",
    "\n",
    "    O = torch.cat(O)\n",
    "    O = O[:N]  # remove padding\n",
    "    return O\n",
    "\n",
    "\n",
    "N = 6  # batch size\n",
    "d = 4  # embed dim (head dim)\n",
    "M = 128  # on-chip SRAM size\n",
    "\n",
    "B_c = cdiv(M, 4 * d)  # block size\n",
    "B_r = min(B_c, d)\n",
    "\n",
    "B_c = B_r = 4\n",
    "\n",
    "\n",
    "Q = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "K = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "V = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "output_flash = torch_flash_attention_kernel(Q, K, V, B_r=B_r, B_c=B_c)\n",
    "# print(F.scaled_dot_product_attention(Q, K, V))\n",
    "output_torch = torch_scaled_dot_product_attention(Q, K, V)\n",
    "print(torch.allclose(output_flash, output_torch, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O_scale tensor([0.4511, 0.3902, 0.4491, 0.4271, 0.5540, 0.6602, 0.5172, 0.4376, 0.3818,\n",
      "        0.5414, 0.4532, 0.5827, 0.4518, 0.6428, 0.5493, 0.3956],\n",
      "       device='cuda:0')\n",
      "True\n",
      "tensor(    0.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def triton_flash_attention_kernel(\n",
    "    Q_ptr,\n",
    "    K_ptr,\n",
    "    V_ptr,\n",
    "    O_ptr,\n",
    "    N: int,\n",
    "    d: int,\n",
    "    softmax_scale: float,\n",
    "    B_r: tl.constexpr,\n",
    "    B_c: tl.constexpr,\n",
    "    B_d: tl.constexpr,\n",
    "    allow_tf32: tl.constexpr = False,\n",
    "):\n",
    "    assert d == B_d\n",
    "    i = tl.program_id(0)\n",
    "\n",
    "    # 8. Load Q_i, O_i, l_i, m_i into SRAM\n",
    "    Q_i_rows = i * B_r + tl.arange(0, B_r)\n",
    "    Q_i_cols = tl.arange(0, B_d)\n",
    "    Q_i_ptrs = Q_ptr + Q_i_rows[:, None] * d + Q_i_cols[None, :]\n",
    "    Q_i_mask = (Q_i_rows[:, None] < N) & (Q_i_cols[None, :] < d)\n",
    "    Q_i = tl.load(Q_i_ptrs, mask=Q_i_mask, other=0.0)  # [B_r, d]\n",
    "\n",
    "    O_i = tl.zeros((B_r, B_c), dtype=Q_i.dtype)  # [B_r, B_c]\n",
    "    l_i = tl.zeros((B_r,), dtype=Q_i.dtype)  # [B_r]\n",
    "    m_i = tl.full((B_r,), -float(\"inf\"), dtype=Q_i.dtype)  # [B_r]\n",
    "\n",
    "    # 3. Divide K, V into T_c blocks of size [B_c, d] each\n",
    "    T_c = tl.cdiv(N, B_c)\n",
    "\n",
    "    # Inner loop (NOTE: in Algorithm 1, this is the outer loop; Algorithm 1's inner loop is the outer loop here via tl.program_id(0))\n",
    "    for j in range(T_c):\n",
    "        # 3. Divide K, V into T_c blocks of size [B_c, d] each\n",
    "        # 6. Load K_j, V_j into SRAM\n",
    "        # NOTE: we actually load K_j.T to avoid the transpose\n",
    "        K_j_rows = j * B_c + tl.arange(0, B_c)\n",
    "        K_j_cols = tl.arange(0, B_d)\n",
    "        K_j_ptrs = K_ptr + K_j_rows[None, :] * d + K_j_cols[:, None]\n",
    "        K_j_mask = (K_j_rows[None, :] < N) & (K_j_cols[:, None] < d)\n",
    "        K_j = tl.load(K_j_ptrs, mask=K_j_mask, other=0.0)  # [B_c, d]\n",
    "\n",
    "        V_j_rows = K_j_rows\n",
    "        V_j_cols = K_j_cols\n",
    "        V_j_ptrs = V_ptr + V_j_rows[:, None] * d + V_j_cols[None, :]\n",
    "        V_j_mask = K_j_mask\n",
    "        V_j = tl.load(V_j_ptrs, mask=V_j_mask, other=0.0)  # [B_c, d]\n",
    "\n",
    "        # 9. On chip, compute S_ij = Q_i @ K_j^T\n",
    "        S_ij = tl.dot(Q_i, K_j, allow_tf32=allow_tf32)  # [B_r, B_c] # NOTE: K_j is already loaded in its transpose\n",
    "\n",
    "        # 9a. Scale by sqrt(d) (not in the paper, but part of the attention formula)\n",
    "        S_ij = S_ij * softmax_scale\n",
    "\n",
    "        # 9b. Mask out-of-bounds elements\n",
    "        S_ij = tl.where((K_j_rows[None, :] < N), S_ij, -float(\"inf\"))\n",
    "\n",
    "        # 10. On chip, compute mtilde_ij = rowmax(S_ij)\n",
    "        mtilde_ij = tl.max(S_ij, axis=1)  # [B_r]\n",
    "\n",
    "        # 10. On chip, compute Ptilde_ij = exp(S_ij - mtilde_ij)\n",
    "        Ptilde_ij = tl.exp(S_ij - mtilde_ij[:, None])  # [B_r, B_c]\n",
    "\n",
    "        # 11. On chip, compute ltilde_ij = rowsum(Ptilde_ij)\n",
    "        ltilde_ij = tl.sum(Ptilde_ij, axis=1)  # [B_r]\n",
    "\n",
    "        # 11. On chip, compute mnew_i = max(m_i, mtilde_ij)\n",
    "        mnew_i = tl.maximum(m_i, mtilde_ij)  # [B_r]\n",
    "\n",
    "        # 11. On chip, compute lnew_i = exp(m_i - mnew_i) * l_i + exp(mtilde_ij - mnew_i) * ltilde_ij\n",
    "        alpha = tl.exp(m_i - mnew_i)  # [B_r]\n",
    "        beta = tl.exp(mtilde_ij - mnew_i)  # [B_r]\n",
    "        lnew_i = alpha * l_i + beta * ltilde_ij  # [B_r]\n",
    "\n",
    "        # 12. Write O_i = diag(lnew_i)^-1 (diag(l_i) exp(m_i - mnew_i) O_i + exp(mtilde_ij - mnew_i) Ptilde_ij V_j) to HBM\n",
    "        P_scale = beta / lnew_i  # [B_r]\n",
    "        O_scale = l_i / lnew_i * alpha  # [B_r]\n",
    "        O_i = O_i * O_scale[:, None] + tl.dot(Ptilde_ij * P_scale[:, None], V_j, allow_tf32=allow_tf32)\n",
    "\n",
    "        # 13. Write l_i = lnew_i to HBM\n",
    "        l_i = lnew_i\n",
    "\n",
    "        # 13. Write m_i = mnew_i to HBM\n",
    "        m_i = mnew_i\n",
    "\n",
    "    # 12. Write O_i to HBM\n",
    "    O_i_rows = Q_i_rows\n",
    "    O_i_cols = Q_i_cols\n",
    "    O_i_ptrs = O_ptr + O_i_rows[:, None] * d + O_i_cols[None, :]\n",
    "    O_i_mask = Q_i_mask\n",
    "    tl.store(O_i_ptrs, O_i, mask=O_i_mask)\n",
    "\n",
    "\n",
    "def triton_flash_attention(Q, K, V, B_r, B_c):\n",
    "    N, d = Q.shape\n",
    "    dtype = Q.dtype\n",
    "\n",
    "    # 2. Initialize O, l, m in HBM\n",
    "    O = torch.zeros(N, d, device=Q.device, dtype=dtype)  # [N, d]\n",
    "\n",
    "    B_d = d\n",
    "\n",
    "    T_r = cdiv(N, B_r)\n",
    "    softmax_scale = 1.0 / d**0.5\n",
    "\n",
    "    triton_flash_attention_kernel[(T_r, 1, 1)](Q, K, V, O, N, d, softmax_scale, B_r, B_c, B_d)\n",
    "    return O\n",
    "\n",
    "\n",
    "N = 32  # batch size\n",
    "d = 16  # embed dim (head dim)\n",
    "M = 128  # on-chip SRAM size\n",
    "\n",
    "B_c = cdiv(M, 4 * d)  # block size\n",
    "B_r = min(B_c, d)\n",
    "\n",
    "B_r = B_c = 16\n",
    "B_d = d\n",
    "\n",
    "torch.manual_seed(0)\n",
    "Q = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "K = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "V = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "output_triton = triton_flash_attention(Q, K, V, B_r=B_r, B_c=B_c)\n",
    "# print(F.scaled_dot_product_attention(Q, K, V))\n",
    "# output_torch = torch_scaled_dot_product_attention(Q, K, V)\n",
    "torch.cuda.synchronize()\n",
    "output_torch = torch_flash_attention_kernel(Q, K, V, B_r=B_r, B_c=B_c)\n",
    "print(torch.allclose(output_triton, output_torch, atol=1e-6))\n",
    "print((output_torch - output_triton).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
