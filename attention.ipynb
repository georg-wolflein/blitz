{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from typing import Optional\n",
    "\n",
    "from attn_torch import torch_attention\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "\n",
    "def cdiv(a, b):\n",
    "    return (a + b - 1) // b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch implementation of FlashAttention\n",
    "I follow the notation of Algorithm 1: https://arxiv.org/pdf/2205.14135.pdf\n",
    "\n",
    "Helpful resources:\n",
    "- [triton implementation of FA1](https://github.com/openai/triton/blob/fdf1c1f2a1f4de37ce1fb31316d53004d6e7e98c/python/tutorials/06-fused-attention.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def pad_to(x, dim, size, value=0.0):\n",
    "    \"\"\"Append padding to the input tensor x to match the target size along the given dimension.\"\"\"\n",
    "    pad_size = size - x.size(dim)\n",
    "    if pad_size > 0:\n",
    "        pad_dims = list(x.shape)\n",
    "        pad_dims[dim] = pad_size\n",
    "        pad = torch.full(pad_dims, value, dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([x, pad], dim=dim)\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def torch_flash_attention_kernel(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    softmax_scale: Optional[float] = None,\n",
    "    B_r: int = 128,\n",
    "    B_c: int = 128,\n",
    "):\n",
    "    \"\"\"Flash attention kernel implementation using torch operations.\n",
    "\n",
    "    This implementation closely follows Algorithm 1 in the FlashAttention paper: https://arxiv.org/pdf/2205.14135.pdf.\n",
    "    The only difference is that we perform the attention scaling by sqrt(d) as part of the computation.\n",
    "\n",
    "    This implementation is not intended to be used; it is only for reference and testing purposes.\n",
    "\n",
    "    Args:\n",
    "        Q: Queries tensor of shape [Z, H, N, D]\n",
    "        K: Keys tensor of shape [Z, H, N, D]\n",
    "        V: Values tensor of shape [Z, H, N, D]\n",
    "        B_r: The block size for the rows\n",
    "        B_c: The block size for the columns\n",
    "    \"\"\"\n",
    "\n",
    "    Z, H, N, D = Q.shape\n",
    "    dtype = Q.dtype\n",
    "    device = Q.device\n",
    "\n",
    "    softmax_scale = softmax_scale or 1.0 / (D**0.5)\n",
    "\n",
    "    def inner(Q, K, V):\n",
    "\n",
    "        # 2. Initialize O in HBM\n",
    "        O = torch.zeros(N, D, device=device, dtype=dtype)  # [N, D]\n",
    "\n",
    "        # 3. Divide Q into T_r blocks of size [B_r, D] each\n",
    "        T_r = cdiv(N, B_r)\n",
    "        Q = list(torch.split(Q, B_r))  # [T_r, B_r, D]\n",
    "\n",
    "        # 3. Divide K, V into T_c blocks of size [B_c, d] each\n",
    "        T_c = cdiv(N, B_c)\n",
    "        K = list(torch.split(K, B_c))  # [T_c, B_c, D]\n",
    "        V = list(torch.split(V, B_c))  # [T_c, B_c, D]\n",
    "\n",
    "        # 4. Divide O into T_r blocks of size [B_r, D] each\n",
    "        O = list()\n",
    "\n",
    "        # 7. Outer loop (NOTE: in Algorithm 1, this is the inner loop)\n",
    "        for i in range(T_r):\n",
    "            # 8. Load Q_i, O_i, l_i, m_i into SRAM\n",
    "            Q_i = Q[i]  # [B_r, D]\n",
    "            Q_i = pad_to(Q_i, 0, B_r)  # simulate padding\n",
    "\n",
    "            # 2. and 4. Divide l, m into T_r blocks of size [B_r] each\n",
    "            l_i = torch.zeros(B_r, device=device, dtype=dtype)  # [B_r]\n",
    "            m_i = torch.full((B_r,), float(\"-inf\"), device=device, dtype=dtype)  # [B_r]\n",
    "            O_i = torch.zeros(B_r, D, device=device, dtype=dtype)  # [B_r, D]\n",
    "\n",
    "            # 5. Inner loop (NOTE: in Algorithm 1, this is the outer loop)\n",
    "            for j in range(T_c):\n",
    "                # 6. Load K_j, V_j into SRAM\n",
    "                K_j = K[j]  # [B_c, d]\n",
    "                V_j = V[j]  # [B_c, d]\n",
    "\n",
    "                K_j = pad_to(K_j, 0, B_c)  # simulate padding\n",
    "                V_j = pad_to(V_j, 0, B_c)  # simulate padding\n",
    "\n",
    "                # 9. On chip, compute S_ij = Q_i @ K_j^T\n",
    "                S_ij = Q_i @ K_j.T  # [B_r, B_c]\n",
    "\n",
    "                # 9a. Scale by sqrt(d) (not in the paper, but part of the attention formula)\n",
    "                S_ij = S_ij * softmax_scale\n",
    "\n",
    "                # 9b. Mask out-of-bounds elements\n",
    "                S_ij = torch.where(torch.arange(B_c, device=device).unsqueeze(0) + j * B_c < N, S_ij, -float(\"inf\"))\n",
    "\n",
    "                # 10. On chip, compute mtilde_ij = rowmax(S_ij)\n",
    "                mtilde_ij = S_ij.max(dim=1).values  # [B_r]\n",
    "\n",
    "                # 10. On chip, compute Ptilde_ij = exp(S_ij - mtilde_ij)\n",
    "                Ptilde_ij = torch.exp(S_ij - mtilde_ij.unsqueeze(1))  # [B_r, B_c]\n",
    "\n",
    "                # 11. On chip, compute ltilde_ij = rowsum(Ptilde_ij)\n",
    "                ltilde_ij = Ptilde_ij.sum(dim=1)  # [B_r]\n",
    "\n",
    "                # 11. On chip, compute mnew_i = max(m_i, mtilde_ij)\n",
    "                mnew_i = torch.maximum(m_i, mtilde_ij)  # [B_r]\n",
    "\n",
    "                # 11. On chip, compute lnew_i = exp(m_i - mnew_i) * l_i + exp(mtilde_ij - mnew_i) * ltilde_ij\n",
    "                alpha = torch.exp(m_i - mnew_i)  # [B_r]\n",
    "                beta = torch.exp(mtilde_ij - mnew_i)  # [B_r]\n",
    "                lnew_i = alpha * l_i + beta * ltilde_ij  # [B_r]\n",
    "\n",
    "                # 12. Write O_i = diag(lnew_i)^-1 (diag(l_i) exp(m_i - mnew_i) O_i + exp(mtilde_ij - mnew_i) Ptilde_ij V_j) to HBM\n",
    "                P_scale = beta / lnew_i  # [B_r]\n",
    "                O_scale = l_i / lnew_i * alpha  # [B_r]\n",
    "                O_i = O_i * O_scale.unsqueeze(1) + (Ptilde_ij * P_scale.unsqueeze(1)) @ V_j\n",
    "\n",
    "                # 13. Write l_i = lnew_i to HBM\n",
    "                l_i = lnew_i\n",
    "\n",
    "                # 13. Write m_i = mnew_i to HBM\n",
    "                m_i = mnew_i\n",
    "\n",
    "            O.append(O_i)  # write to HBM\n",
    "\n",
    "        O = torch.cat(O)\n",
    "        O = O[:N]  # remove padding\n",
    "        return O\n",
    "\n",
    "    # Run inner across Z, H dimensions\n",
    "    O = torch.stack([torch.stack([inner(Q[z, h], K[z, h], V[z, h]) for h in range(H)]) for z in range(Z)])\n",
    "    return O\n",
    "\n",
    "\n",
    "Z = 6  # batch size\n",
    "H = 2  # num heads\n",
    "N = 8  # sequence length\n",
    "D = 4  # embed dim (head dim)\n",
    "\n",
    "# M = 128  # on-chip SRAM size\n",
    "# B_c = cdiv(M, 4 * d)  # block size\n",
    "# B_r = min(B_c, d)\n",
    "\n",
    "B_c = B_r = 4\n",
    "\n",
    "B_r = 16\n",
    "B_c = 16\n",
    "Z = 17\n",
    "H = 2\n",
    "N = 33\n",
    "D = 64\n",
    "\n",
    "\n",
    "Q = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "K = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "V = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "output_flash = torch_flash_attention_kernel(Q, K, V, B_r=B_r, B_c=B_c)\n",
    "# print(F.scaled_dot_product_attention(Q, K, V))\n",
    "output_torch = torch_attention(Q, K, V)\n",
    "print(torch.allclose(output_flash, output_torch, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton autotuning for function triton_flash_attention_kernel finished after 4169.58s; best config selected: B_r: 16, B_c: 128, num_warps: 2, num_ctas: 1, num_stages: 4;\n",
      "True\n",
      "tensor(    0.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# @triton.autotune(\n",
    "#     [\n",
    "#         triton.Config({\"B_r\": B_r, \"B_c\": B_c}, num_warps=num_warps, num_stages=num_stages)\n",
    "#         for B_r in [16, 32, 64, 128]\n",
    "#         for B_c in [16, 32, 64, 128]\n",
    "#         for num_warps in [2, 4, 8]\n",
    "#         for num_stages in [3, 4, 5]\n",
    "#     ],\n",
    "#     key=[\"Z\", \"H\", \"N\", \"D\"],\n",
    "# )\n",
    "@triton.jit\n",
    "def triton_flash_attention_kernel(\n",
    "    Q_ptr,\n",
    "    K_ptr,\n",
    "    V_ptr,\n",
    "    O_ptr,\n",
    "    stride_QZ,\n",
    "    stride_QH,\n",
    "    stride_QN,\n",
    "    stride_QD,\n",
    "    stride_KZ,\n",
    "    stride_KH,\n",
    "    stride_KN,\n",
    "    stride_KD,\n",
    "    stride_VZ,\n",
    "    stride_VH,\n",
    "    stride_VN,\n",
    "    stride_VD,\n",
    "    stride_OZ,\n",
    "    stride_OH,\n",
    "    stride_ON,\n",
    "    stride_OD,\n",
    "    Z: int,  # batch size\n",
    "    H: int,  # number of heads\n",
    "    N: int,  # sequence length\n",
    "    D: int,  # embedding dimension (per head)\n",
    "    softmax_scale: float,\n",
    "    B_r: tl.constexpr,\n",
    "    B_c: tl.constexpr,\n",
    "    B_d: tl.constexpr,\n",
    "    allow_tf32: tl.constexpr = False,\n",
    "):\n",
    "    assert D == B_d\n",
    "\n",
    "    # Index into outer loop (inner loop in Algorithm 1)\n",
    "    i = tl.program_id(0)\n",
    "\n",
    "    # Find the correct start position for this block in terms of the Z, H dimensions (batch and head dimensions)\n",
    "    zh = tl.program_id(1)\n",
    "    z = zh // H\n",
    "    h = zh % H\n",
    "    Q_ptr = Q_ptr + z.to(tl.int64) * stride_QZ + h.to(tl.int64) * stride_QH\n",
    "    K_ptr = K_ptr + z.to(tl.int64) * stride_KZ + h.to(tl.int64) * stride_KH\n",
    "    V_ptr = V_ptr + z.to(tl.int64) * stride_VZ + h.to(tl.int64) * stride_VH\n",
    "    O_ptr = O_ptr + z.to(tl.int64) * stride_OZ + h.to(tl.int64) * stride_OH\n",
    "\n",
    "    # 8. Load Q_i into SRAM; will sty in SRAM throughout this block\n",
    "    Q_i_ptrs = tl.make_block_ptr(\n",
    "        base=Q_ptr,\n",
    "        shape=(N, D),\n",
    "        strides=(stride_QN, stride_QD),\n",
    "        offsets=(i * B_r, 0),\n",
    "        block_shape=(B_r, B_d),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    Q_i = tl.load(Q_i_ptrs, boundary_check=(0, 1))  # [B_r, D]\n",
    "\n",
    "    # Initialize local O_i, l_i, m_i for this block\n",
    "    O_i = tl.zeros((B_r, B_d), dtype=Q_i.dtype)  # [B_r, D]\n",
    "    l_i = tl.zeros((B_r,), dtype=Q_i.dtype)  # [B_r]\n",
    "    m_i = tl.full((B_r,), -float(\"inf\"), dtype=Q_i.dtype)  # [B_r]\n",
    "\n",
    "    # 3. Divide K, V into T_c blocks of size [B_c, D] each\n",
    "    T_c = tl.cdiv(N, B_c)\n",
    "\n",
    "    # We only prepare the block pointer here; loading/adavancing will be done in the inner loop\n",
    "    K_j_ptrs = tl.make_block_ptr(\n",
    "        base=K_ptr,\n",
    "        shape=(D, N),\n",
    "        strides=(stride_KD, stride_KN),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(B_d, B_c),\n",
    "        order=(1, 0),\n",
    "    )  # NOTE: we are loading K_j^T, so the strides and order are swapped\n",
    "    V_j_ptrs = tl.make_block_ptr(\n",
    "        base=V_ptr,\n",
    "        shape=(N, D),\n",
    "        strides=(stride_VN, stride_VD),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(B_c, B_d),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "\n",
    "    # Inner loop (NOTE: in Algorithm 1, this is the outer loop; Algorithm 1's inner loop is the outer loop here via tl.program_id(0))\n",
    "    for j in range(T_c):\n",
    "        # 3. Divide K, V into T_c blocks of size [B_c, D] each\n",
    "        # 6. Load K_j, V_j into SRAM\n",
    "        K_j = tl.load(K_j_ptrs, boundary_check=(1, 0))  # [D, B_c] # NOTE: K_j is loaded in its transpose\n",
    "        V_j = tl.load(V_j_ptrs, boundary_check=(0, 1))  # [B_c, D]\n",
    "\n",
    "        # 9. On chip, compute S_ij = Q_i @ K_j^T\n",
    "        S_ij = tl.dot(Q_i, K_j, allow_tf32=allow_tf32)  # [B_r, B_c] # NOTE: K_j is already loaded in its transpose\n",
    "\n",
    "        # 9a. Scale by sqrt(d) (not in the paper, but part of the attention formula)\n",
    "        S_ij = S_ij * softmax_scale\n",
    "\n",
    "        # 9b. Mask out-of-bounds elements\n",
    "        rows = j * B_c + tl.arange(0, B_c)\n",
    "        S_ij = tl.where((rows[None, :] < N), S_ij, -float(\"inf\"))\n",
    "\n",
    "        # 10. On chip, compute mtilde_ij = rowmax(S_ij)\n",
    "        mtilde_ij = tl.max(S_ij, axis=1)  # [B_r]\n",
    "\n",
    "        # 10. On chip, compute Ptilde_ij = exp(S_ij - mtilde_ij)\n",
    "        Ptilde_ij = tl.exp(S_ij - mtilde_ij[:, None])  # [B_r, B_c]\n",
    "\n",
    "        # 11. On chip, compute ltilde_ij = rowsum(Ptilde_ij)\n",
    "        ltilde_ij = tl.sum(Ptilde_ij, axis=1)  # [B_r]\n",
    "\n",
    "        # 11. On chip, compute mnew_i = max(m_i, mtilde_ij)\n",
    "        mnew_i = tl.maximum(m_i, mtilde_ij)  # [B_r]\n",
    "\n",
    "        # 11. On chip, compute lnew_i = exp(m_i - mnew_i) * l_i + exp(mtilde_ij - mnew_i) * ltilde_ij\n",
    "        alpha = tl.exp(m_i - mnew_i)  # [B_r]\n",
    "        beta = tl.exp(mtilde_ij - mnew_i)  # [B_r]\n",
    "        lnew_i = alpha * l_i + beta * ltilde_ij  # [B_r]\n",
    "\n",
    "        # 12. Write O_i = diag(lnew_i)^-1 (diag(l_i) exp(m_i - mnew_i) O_i + exp(mtilde_ij - mnew_i) Ptilde_ij V_j) to HBM\n",
    "        P_scale = beta / lnew_i  # [B_r]\n",
    "        O_scale = l_i / lnew_i * alpha  # [B_r]\n",
    "        O_i = O_i * O_scale[:, None] + tl.dot(Ptilde_ij * P_scale[:, None], V_j, allow_tf32=allow_tf32)\n",
    "\n",
    "        # 13. Write l_i = lnew_i to HBM\n",
    "        l_i = lnew_i\n",
    "\n",
    "        # 13. Write m_i = mnew_i to HBM\n",
    "        m_i = mnew_i\n",
    "\n",
    "        # Advance block pointers to the next block\n",
    "        K_j_ptrs = K_j_ptrs.advance((0, B_c))  # NOTE: K_j is loaded in its transpose\n",
    "        V_j_ptrs = V_j_ptrs.advance((B_c, 0))\n",
    "\n",
    "    # 12. Write O_i to HBM\n",
    "    O_i_ptrs = tl.make_block_ptr(\n",
    "        base=O_ptr,\n",
    "        shape=(N, D),\n",
    "        strides=(stride_ON, stride_OD),\n",
    "        offsets=(i * B_r, 0),\n",
    "        block_shape=(B_r, B_d),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    tl.store(O_i_ptrs, O_i, boundary_check=(0, 1))\n",
    "\n",
    "\n",
    "def triton_flash_attention(Q, K, V, B_r=128, B_c=128):\n",
    "    Z, H, N, D = Q.shape\n",
    "    dtype = Q.dtype\n",
    "\n",
    "    # 2. Initialize O, l, m in HBM\n",
    "    O = torch.zeros(Z, H, N, D, device=Q.device, dtype=dtype)  # [N, d]\n",
    "\n",
    "    B_d = triton.next_power_of_2(D)\n",
    "    B_r = 128\n",
    "    B_c = 64 if D <= 64 else 32\n",
    "    num_stages = 4 if D <= 64 else 3\n",
    "    num_warps = 4\n",
    "\n",
    "    T_r = cdiv(N, B_r)\n",
    "    softmax_scale = 1.0 / D**0.5\n",
    "\n",
    "    grid = (T_r, Z * H)\n",
    "\n",
    "    triton_flash_attention_kernel[grid](\n",
    "        Q,\n",
    "        K,\n",
    "        V,\n",
    "        O,\n",
    "        Q.stride(0),\n",
    "        Q.stride(1),\n",
    "        Q.stride(2),\n",
    "        Q.stride(3),\n",
    "        K.stride(0),\n",
    "        K.stride(1),\n",
    "        K.stride(2),\n",
    "        K.stride(3),\n",
    "        V.stride(0),\n",
    "        V.stride(1),\n",
    "        V.stride(2),\n",
    "        V.stride(3),\n",
    "        O.stride(0),\n",
    "        O.stride(1),\n",
    "        O.stride(2),\n",
    "        O.stride(3),\n",
    "        Z,\n",
    "        H,\n",
    "        N,\n",
    "        D,\n",
    "        softmax_scale,\n",
    "        B_d=B_d,\n",
    "        B_r=B_r,\n",
    "        B_c=B_c,\n",
    "        num_stages=num_stages,\n",
    "        num_warps=num_warps,\n",
    "    )\n",
    "    return O\n",
    "\n",
    "\n",
    "Z = 1  # batch size\n",
    "H = 8  # num heads\n",
    "N = 1024  # sequence length\n",
    "D = 64  # embed dim (head dim)\n",
    "\n",
    "\n",
    "# B_c = cdiv(M, 4 * d)  # block size\n",
    "# B_r = min(B_c, d)\n",
    "\n",
    "\n",
    "# N = 2\n",
    "# D = 16\n",
    "# B_r = B_c = 16\n",
    "# B_d = D\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "Q = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "K = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "V = torch.randn(Z, H, N, D, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "output_triton = triton_flash_attention(Q, K, V)\n",
    "# print(F.scaled_dot_product_attention(Q, K, V))\n",
    "# output_torch = torch_scaled_dot_product_attention(Q, K, V)\n",
    "torch.cuda.synchronize()\n",
    "output_torch = torch_flash_attention_kernel(Q, K, V)\n",
    "print(torch.allclose(output_triton, output_torch, atol=1e-6))\n",
    "print((output_torch - output_triton).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
