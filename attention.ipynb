{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRITON_INTERPRETED\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton_utils import test_pid_conds, breakpoint_if, print_if, check_tensors_gpu_ready, cdiv\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_scaled_dot_product_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "    \"\"\"Reference implementation of scaled dot product attention using torch operations.\n",
    "\n",
    "    Args:\n",
    "        q: Queries tensor of shape [B, H, S, D_k]\n",
    "        k: Keys tensor of shape [B, H, S, D_k]\n",
    "        v: Values tensor of shape [B, H, S, D_v]\n",
    "\n",
    "    Returns:\n",
    "        values: The output of the attention mechanism of shape [B, H, S, D_v]\n",
    "        attn_logits: The attention logits of shape [B, H, S, S]\n",
    "        attention: The attention weights of shape [B, H, S, S]\n",
    "\n",
    "    Shapes:\n",
    "        B: batch size\n",
    "        H: number of heads\n",
    "        S: sequence length\n",
    "        D_k: key dimension\n",
    "        D_v: value dimension\n",
    "    \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = q @ k.transpose(-2, -1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "    attn_logits = attn_logits / d_k**0.5\n",
    "    attention = F.softmax(attn_logits, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "    values = attention @ v  # [batch_size, num_heads, seq_len, embed_dim]\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7, 4])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_on_dim(tensor, size, dim, value=0):\n",
    "    \"\"\"Pads a tensor with zeros on a given dimension.\n",
    "\n",
    "    Args:\n",
    "        tensor: The input tensor\n",
    "        size: The size that the dimension should be padded to\n",
    "        dim: The dimension to pad\n",
    "        value: The value to pad with\n",
    "\n",
    "    Returns:\n",
    "        The padded tensor\n",
    "    \"\"\"\n",
    "    pad_shape = list(tensor.shape)\n",
    "    pad_shape[dim] = size - pad_shape[dim]\n",
    "    # print(f\"{tensor.shape=}, {pad_shape=}, {dim=}, {size=}\")\n",
    "    assert pad_shape[dim] >= 0\n",
    "    return torch.cat([tensor, torch.full(pad_shape, value, dtype=tensor.dtype, device=tensor.device)], dim=dim)\n",
    "\n",
    "\n",
    "x = torch.randn(1, 1, 4, 4, dtype=torch.float32, device=\"cuda\")\n",
    "pad_on_dim(x, 7, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch implementation of FlashAttention\n",
    "I follow the notation of Algorithm 1: https://arxiv.org/pdf/2205.14135.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def torch_flash_attention_kernel(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, B_r: int, B_c: int):\n",
    "    \"\"\"Flash attention kernel implementation using torch operations.\n",
    "\n",
    "    This implementation closely follows Algorithm 1 in the FlashAttention paper: https://arxiv.org/pdf/2205.14135.pdf.\n",
    "    Unlike Algorithm 1, this implementation performs the scaling by sqrt(d) as part of the attention formula.\n",
    "\n",
    "    Args:\n",
    "        Q: Queries tensor of shape [N, d]\n",
    "        K: Keys tensor of shape [N, d]\n",
    "        V: Values tensor of shape [N, d]\n",
    "        B_r: The block size for the rows\n",
    "        B_c: The block size for the columns\n",
    "    \"\"\"\n",
    "\n",
    "    N, d = Q.shape\n",
    "    dtype = Q.dtype\n",
    "\n",
    "    # 2. Initialize O, l, m in HBM\n",
    "    O = torch.zeros(N, d, device=Q.device, dtype=dtype)  # [N, d]\n",
    "    l = torch.zeros(N, device=Q.device, dtype=dtype)  # [N]\n",
    "    m = torch.full((N,), float(\"-inf\"), device=Q.device, dtype=dtype)  # [N]\n",
    "\n",
    "    # 3. Divide Q into T_r blocks of size [B_r, d] each\n",
    "    T_r = cdiv(N, B_r)\n",
    "    Q = list(torch.split(Q, B_r))  # [T_r, B_r, d]\n",
    "\n",
    "    # 3. Divide K, V into T_c blocks of size [B_c, d] each\n",
    "    T_c = cdiv(N, B_c)\n",
    "    K = list(torch.split(K, B_c))  # [T_c, B_c, d]\n",
    "    V = list(torch.split(V, B_c))  # [T_c, B_c, d]\n",
    "\n",
    "    # 4. Divide O into T_r blocks of size [B_r, d] each\n",
    "    O = list(torch.split(O, B_r))  # [T_r, B_r, d]\n",
    "\n",
    "    # 4. Divide l into T_r blocks of size [B_r] each\n",
    "    l = list(torch.split(l, B_r))  # [T_r, B_r]\n",
    "\n",
    "    # 4. Divide m into T_r blocks of size [B_r] each\n",
    "    m = list(torch.split(m, B_r))  # [T_r, B_r]\n",
    "\n",
    "    # 5. Outer loop\n",
    "    for j in range(T_c):\n",
    "        # 6. Load K_j, V_j into SRAM\n",
    "        K_j = K[j]  # [B_c, d]\n",
    "        V_j = V[j]  # [B_c, d]\n",
    "\n",
    "        # 7. Inner loop\n",
    "        for i in range(T_r):\n",
    "            # 8. Load Q_i, O_i, l_i, m_i into SRAM\n",
    "            Q_i = Q[i]  # [B_r, d]\n",
    "            O_i = O[i]  # [B_r, d]\n",
    "            l_i = l[i]  # [B_r]\n",
    "            m_i = m[i]  # [B_r]\n",
    "\n",
    "            # 9. On chip, compute S_ij = Q_i @ K_j^T\n",
    "            S_ij = Q_i @ K_j.T  # [B_r, B_c]\n",
    "\n",
    "            # 9a. Scale by sqrt(d) (not in the paper, but part of the attention formula)\n",
    "            S_ij = S_ij / (d**0.5)\n",
    "\n",
    "            # 10. On chip, compute mtilde_ij = rowmax(S_ij)\n",
    "            mtilde_ij = S_ij.max(dim=1).values  # [B_r]\n",
    "\n",
    "            # 10. On chip, compute Ptilde_ij = exp(S_ij - mtilde_ij)\n",
    "            Ptilde_ij = torch.exp(S_ij - mtilde_ij.unsqueeze(1))  # [B_r, B_c]\n",
    "\n",
    "            # 11. On chip, compute ltilde_ij = rowsum(Ptilde_ij)\n",
    "            ltilde_ij = Ptilde_ij.sum(dim=1)  # [B_r]\n",
    "\n",
    "            # 11. On chip, compute mnew_i = max(m_i, mtilde_ij)\n",
    "            mnew_i = torch.maximum(m_i, mtilde_ij)  # [B_r]\n",
    "\n",
    "            # 11. On chip, compute lnew_i = exp(m_i - mnew_i) * l_i + exp(mtilde_ij - mnew_i) * ltilde_ij\n",
    "            lnew_i = torch.exp(m_i - mnew_i) * l_i + torch.exp(mtilde_ij - mnew_i) * ltilde_ij  # [B_r]\n",
    "\n",
    "            # 12. Write O_i = diag(lnew_i)^-1 (diag(l_i) exp(m_i - mnew_i) O_i + exp(mtilde_ij - mnew_i) Ptilde_ij V_j) to HBM\n",
    "            #           O_i = a @ (b + c)\n",
    "            #           where:\n",
    "            #             a = diag(lnew_i) ** -1                             [B_r, B_r]\n",
    "            #             b = diag(l_i) * exp(m_i - mnew_i) @ O_i            [B_r, d]\n",
    "            #                 [B_r, B_r]  [B_r]               [B_r, d]\n",
    "            #             c = exp(mtilde_ij - mnew_i) * Ptilde_ij @ V_j      [B_r, d]\n",
    "            #                 [B_r]                     [B_r, B_c]  [B_c, d]\n",
    "            _a = torch.diag(lnew_i**-1)  # [B_r, B_r]\n",
    "            _b = torch.diag(l_i) * torch.exp(m_i - mnew_i) @ O_i  # [B_r, d]\n",
    "            _c = torch.exp(mtilde_ij - mnew_i).unsqueeze(1) * Ptilde_ij @ V_j\n",
    "            O_i = _a @ (_b + _c)\n",
    "            O[i] = O_i  # write to HBM\n",
    "\n",
    "            # 13. Write l_i = lnew_i to HBM\n",
    "            l_i = lnew_i\n",
    "            l[i] = l_i  # write to HBM\n",
    "\n",
    "            # 13. Write m_i = mnew_i to HBM\n",
    "            m_i = mnew_i\n",
    "            m[i] = m_i  # write to HBM\n",
    "\n",
    "    O = torch.cat(O)\n",
    "    return O\n",
    "\n",
    "\n",
    "N = 6  # batch size\n",
    "d = 4  # embed dim (head dim)\n",
    "M = 128  # on-chip SRAM size\n",
    "\n",
    "B_c = cdiv(M, 4 * d)  # block size\n",
    "B_r = min(B_c, d)\n",
    "\n",
    "B_c = 5\n",
    "B_r = 5\n",
    "\n",
    "Q = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "K = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "V = torch.randn(N, d, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "output_flash = torch_flash_attention_kernel(Q, K, V, B_r, B_c)\n",
    "# print(F.scaled_dot_product_attention(Q, K, V))\n",
    "output_torch = torch_scaled_dot_product_attention(Q, K, V)\n",
    "print(torch.allclose(output_flash, output_torch, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "B = 2  # batch size\n",
    "S = 4  # sequence length\n",
    "H = 2  # number of heads\n",
    "D_k = 8  # key embed dimension\n",
    "D_v = 16  # value embed dimension\n",
    "\n",
    "# Query, key, value\n",
    "q = torch.rand(B, H, S, D_k)\n",
    "k = torch.rand(B, H, S, D_k)\n",
    "v = torch.rand(B, H, S, D_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes equal: True\n",
      "Output equal: True\n"
     ]
    }
   ],
   "source": [
    "# Attention\n",
    "# output_torch = F.scaled_dot_product_attention(q, k, v)\n",
    "output_torch, *_ = torch_scaled_dot_product_attention(q, k, v)\n",
    "output_triton = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "# Compare\n",
    "print(\"Shapes equal:\", output_torch.shape == output_triton.shape)\n",
    "print(\"Output equal:\", torch.allclose(output_torch, output_triton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
